# ============================================================================
# Kafka Connect HDFS 3 Sink Connector Configuration
# Track 2A: Apache Kafka Ecosystem
# ============================================================================

# Connector name
name=hdfs3-sink-connector

# Connector class
connector.class=io.confluent.connect.hdfs3.Hdfs3SinkConnector

# Number of tasks
tasks.max=1

# Topics to consume from
topics=tweets_topic

# HDFS Configuration
hdfs.url=hdfs://localhost:9000
hadoop.conf.dir=/etc/hadoop/conf
hadoop.home=/opt/hadoop

# Output directory in HDFS
topics.dir=/project/streamed_tweets_avro

# Logs directory for recovery
logs.dir=/project/connect-logs

# Format - Avro
format.class=io.confluent.connect.hdfs3.avro.AvroFormat

# Schema Registry (for Avro)
# schema.registry.url=http://localhost:8081

# Partitioner - Daily partitioning
partitioner.class=io.confluent.connect.storage.partitioner.DailyPartitioner
path.format='dt'=YYYY-MM-dd
locale=en-US
timezone=UTC

# Flush settings
flush.size=100
rotate.interval.ms=60000

# Retry settings
retry.backoff.ms=5000

# Key and Value converters
key.converter=org.apache.kafka.connect.storage.StringConverter
value.converter=io.confluent.connect.avro.AvroConverter
value.converter.schema.registry.url=http://localhost:8081

# If not using Schema Registry, use JSON converter:
# value.converter=org.apache.kafka.connect.json.JsonConverter
# value.converter.schemas.enable=false

# Storage class
storage.class=io.confluent.connect.hdfs3.Hdfs3Storage

